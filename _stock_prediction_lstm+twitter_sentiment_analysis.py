# -*- coding: utf-8 -*-
"""_Stock_Prediction_Lstm+Twitter_Sentiment_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t5nNn9RSPXUwD4XkMBNh-_u5knnBN43q
"""

# prompt: mount drive

from google.colab import drive
drive.mount('/content/drive')

"""# Introduction

<a id="section-one"></a>
# Import packages
"""

import os
import numpy as np
import csv
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.dates import DateFormatter
import math
import time
import tensorflow as tf
from tensorflow.keras.layers import GRU, LSTM, Bidirectional, Dense, Flatten, Conv1D, BatchNormalization, LeakyReLU, Dropout
from tensorflow.keras import Sequential
from tensorflow.keras.utils import plot_model
from pickle import load
from sklearn.metrics import mean_squared_error
from tqdm import tqdm
!pip install statsmodels
import statsmodels.api as sm
from math import sqrt
from datetime import datetime, timedelta
from sklearn.preprocessing import MinMaxScaler
from pickle import dump
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import unicodedata

"""<a id="section-two"></a>
# Get weekly sentiment for stock ticker

There is much more to the process of stock price formation than plain historical data. Over 1 day, one online post might be a turning point in the course of events, which may result in the market crash. Elon Musk tweets, coronavirus, start of russian full-scale invasion of Ukraine are the proof to that. Therefore, we will take into account another important external indicator, such as the mood of stock market participants. The most effective method in this task is the analysis of the tone (sentiment analysis) of the text, in this notewook we will be consider posts in the social network Twitter.
"""

!pip install contractions

import re
import contractions
# Define the preprocessing steps
def preprocess_text (text) :

  text = text.lower()

  text = re.sub(r'https?:\/V.*[\r\n]*',' ', text)
  text = re.sub(r'www.*[\r\n]*','', text)
  text = re.sub(r'https', ' ', text)
  text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)

  text = re. sub(r'\@\w+','',text)
  text = re.sub(r'\#\w+', '', text)
  text = re.sub(r'@[a-z0-9]+', '', text)
  text = re.sub(r'#[a-z0-9]', '', text)
  text = re.sub(r'@', '', text)
  text = re.sub(r'#', '', text)

  text = text.encode('ascii', 'ignore').decode('ascii')

  text = contractions.fix(text)

  text = text. replace('%', ' percent')
  text = re. sub(r'&amp',
  'and ', text)
  text = re.sub(r'&gt', ' greater than ', text)

  text = re. sub(r'p&f', 'point and figure', text)
  text = re. sub(r's&p', 'SP500', text)
  text = re. sub(r'q&a', 'question and answer', text)
  text = re.sub(r'b/c', ' because ', text)
  text = re.sub(r'b/o', ' break out ', text)
  text = re. sub(r'p/e',
  'pe ratio', text)
  text = re.sub(r'\$[a-zA-Z]+', 'stock', text, flags=re.IGNORECASE)

  return text

stock_name = 'AAPL'

all_tweets = pd.read_csv('/content/drive/MyDrive/sentiment/stock_tweets.csv')

print(all_tweets.shape)
all_tweets.head()





df = all_tweets[all_tweets['Stock Name'] == stock_name]
print(df.shape)
sent_df = df.copy()
df.head()



!pip install transformers torch

"""### TWEEK THE THRESHOLD *VALUES*

> this is NLTK based sentiment model



"""

# import pandas as pd
# import numpy as np
# from nltk.corpus import sentiwordnet as swn
# from nltk.corpus import wordnet as wn
# from nltk import download, pos_tag, word_tokenize
# from nltk.stem import WordNetLemmatizer
# from scipy.special import softmax

# # Download necessary NLTK resources
# download('sentiwordnet')
# download('wordnet')
# download('averaged_perceptron_tagger')
# download('punkt')

# # Initialize WordNet Lemmatizer
# lemmatizer = WordNetLemmatizer()

# # Define weightages for mood states
# mood_weights = {
#     'Calm': 0.2,
#     'Happy': 0.4,
#     'Alert': 0.3,
#     'Kind': 0.1
# }

# # Function to convert NLTK POS tags to WordNet POS tags
# def get_wordnet_pos(treebank_tag):
#     if treebank_tag.startswith('J'):
#         return wn.ADJ
#     elif treebank_tag.startswith('V'):
#         return wn.VERB
#     elif treebank_tag.startswith('N'):
#         return wn.NOUN
#     elif treebank_tag.startswith('R'):
#         return wn.ADV
#     else:
#         return None

# # Function to get sentiment score for a word
# def get_sentiment_score(word, pos):
#     try:
#         # Lemmatize the word with its POS tag
#         word = lemmatizer.lemmatize(word, pos) if pos else word
#         # Get synsets for the word
#         synsets = list(swn.senti_synsets(word, pos))
#         if synsets:
#             synset = synsets[0]
#             # Net sentiment score (positive - negative)
#             return synset.pos_score() - synset.neg_score()
#         return 0.0
#     except Exception:
#         return 0.0

# # Function to assign mood state using softmax
# def assign_mood_state(sentiment_score):
#     # Define raw mood scores based on sentiment
#     if sentiment_score > 0.3:
#         mood_scores = {'Calm': 0.1, 'Happy': 0.7, 'Alert': 0.1, 'Kind': 0.3}
#     elif 0.0 <= sentiment_score <= 0.3:
#         mood_scores = {'Calm': 0.7, 'Happy': 0.2, 'Alert': 0.1, 'Kind': 0.0}
#     elif -0.3 <= sentiment_score < 0.0:
#         mood_scores = {'Calm': 0.6, 'Happy': 0.1, 'Alert': 0.3, 'Kind': 0.0}
#     elif -0.6 <= sentiment_score < -0.3:
#         mood_scores = {'Calm': 0.0, 'Happy': 0.0, 'Alert': 0.2, 'Kind': 0.8}
#     else:  # sentiment_score < -0.6
#         mood_scores = {'Calm': 0.0, 'Happy': 0.0, 'Alert': 0.9, 'Kind': 0.1}

#     # Convert raw scores to probabilities using softmax
#     mood_values = np.array(list(mood_scores.values()))
#     mood_probabilities = softmax(mood_values)

#     # Create a dictionary with mood probabilities
#     mood_state = dict(zip(mood_scores.keys(), mood_probabilities))
#     return mood_state

# # Analyze sentiment for each tweet
# def analyze_sentiment(df):
#     sentiments = []
#     mood_states = {'Calm': [], 'Happy': [], 'Alert': [], 'Kind': []}

#     for tweet in df['Tweet']:
#         tokens = word_tokenize(tweet)
#         tagged = pos_tag(tokens)
#         sentiment = 0

#         # Calculate sentiment score for the tweet
#         for word, tag in tagged:
#             wn_pos = get_wordnet_pos(tag)
#             if wn_pos:
#                 sentiment += get_sentiment_score(word, wn_pos)

#         # Assign mood state using softmax
#         mood_state = assign_mood_state(sentiment)

#         # Calculate weighted sentiment score
#         weighted_sentiment = sum([mood_state[mood] * mood_weights[mood] for mood in mood_state])
#         sentiments.append(weighted_sentiment)

#         # Append mood state probabilities to corresponding lists
#         for mood, value in mood_state.items():
#             mood_states[mood].append(value)

#     # Add sentiment score and mood states to the DataFrame
#     df['Sentiment_Score'] = sentiments
#     df['Calm'] = mood_states['Calm']
#     df['Happy'] = mood_states['Happy']
#     df['Alert'] = mood_states['Alert']
#     df['Kind'] = mood_states['Kind']

#     return df


# # Perform sentiment analysis
# sent_df = analyze_sentiment(sent_df)

# # Output the updated DataFrame with weighted sentiment scores and mood states
# print(sent_df[['Tweet', 'Sentiment_Score', 'Calm', 'Happy', 'Alert', 'Kind']])

"""

> This is a BERT based sentiment model.

"""

import pandas as pd
import numpy as np
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from scipy.special import softmax


MODEL_NAME = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
model = BertForSequenceClassification.from_pretrained(MODEL_NAME)

# Set the device (GPU if available, otherwise CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# Function to preprocess text and get sentiment score using BERT
def get_bert_sentiment(text):
    # Tokenize the text input and convert to input IDs
    tweet = preprocess_text(text)
    inputs = tokenizer(tweet, return_tensors="pt", truncation=True, padding=True, max_length=64)
    inputs = {key: value.to(device) for key, value in inputs.items()}

    # Get the model's output
    with torch.no_grad():
        outputs = model(**inputs)

    # Extract logits and apply softmax to get probabilities
    logits = outputs.logits
    probabilities = softmax(logits.cpu().numpy()[0])

    # Calculate a sentiment score (weighted average of class probabilities)
    sentiment_score = sum([i * prob for i, prob in enumerate(probabilities)]) / (len(probabilities) - 1)

    # Scale the sentiment score to range [-1, 1]
    scaled_score = 2 * (sentiment_score - 0.5)
    return scaled_score

# Function to analyze sentiment for the DataFrame
def analyze_sentiment_with_bert(df):
    sentiments = []

    for tweet in df['Tweet']:
        sentiment = get_bert_sentiment(tweet)
        sentiments.append(sentiment)

    # Add the sentiment scores to the DataFrame
    df['Sentiment_Score'] = sentiments
    return df


# Perform sentiment analysis using BERT on CPU
sent_df = analyze_sentiment_with_bert(sent_df)

# Output the updated DataFrame with sentiment scores
print(sent_df[['Tweet', 'Sentiment_Score']])

"""cell run time - 6m 26s : GPU"""

sent_df.head()

sent_df.info()

# sent_df['Sentiment_Score'] = pd.to_numeric(sent_df['Sentiment_Score'], errors='coerce')
sent_df['Date'] = pd.to_datetime(sent_df['Date'], errors='coerce')

# Verify the conversion
print(sent_df['Date'].dtype)

daily_sentiment = sent_df.groupby(sent_df['Date'].dt.date).agg({

    'Sentiment_Score': 'mean'
}).reset_index()

daily_sentiment.head()

# Plotting the distribution of 'Sentiment_Score'
plt.figure(figsize=(10, 6))
plt.hist(daily_sentiment['Sentiment_Score'], bins=100, color='skyblue', edgecolor='black')
plt.title('Distribution of Daily Sentiment Score')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()



daily_sentiment['Date'] = pd.to_datetime(daily_sentiment['Date'], errors='coerce')


print(daily_sentiment['Date'].dtype)

"""<a id="section-three"></a>
# Get final dataset for training
"""

all_stocks = pd.read_csv('/content/drive/MyDrive/sentiment/stock_yfinance_data.csv')
print(all_stocks.shape)
all_stocks.head()

stock_df = all_stocks[all_stocks['Stock Name'] == stock_name]
stock_df['Date'] = pd.to_datetime(stock_df['Date'])

# Plot the sentiment score and closing price
fig, ax1 = plt.subplots(figsize=(15, 8))
start_date = "2022-01-01"
end_date = "2022-08-31"
# Plot Closing Price on the primary y-axis
ax1.plot(stock_df['Date'], stock_df['Close'], color='b', label='Close')
ax1.set_xlabel('Date')
ax1.set_ylabel('Closing Price USD', color='b')
ax1.tick_params(axis='y', labelcolor='b')
ax1.set_xlim(pd.to_datetime(start_date), pd.to_datetime(end_date))

# Create a secondary y-axis for the sentiment score
ax2 = ax1.twinx()
ax2.plot(daily_sentiment['Date'], daily_sentiment['Sentiment_Score'], color='g', label='Sentiment Score')
ax2.set_ylabel('Sentiment Score', color='r')
ax2.tick_params(axis='y', labelcolor='r')
ax2.axhline(y=0, color='gray', linestyle='--', linewidth=1)
# Add legends
fig.legend(loc='upper left', bbox_to_anchor=(0.1,0.9))

# Display the plot
plt.title('Sentiment Score and opening Price Over Time')
plt.show()

stock_df.info()

final_df = pd.merge(stock_df, daily_sentiment, left_on='Date', right_on='Date', how='inner')

final_df.head()

from google.colab import files
final_df.to_csv('final_df_1_3.csv', encoding='utf-8', index=False)
files.download('final_df_1_3.csv')

"""Let's plot historical price data for the analyzed stock ticker:"""

import pandas as pd
from google.colab import files




final_df = pd.read_csv("/content/final_df_1_3.csv")
  # final_df = pd.read_csv(fn)

# final_df = pd.read_csv('/content/drive/MyDrive/sentiment/final_df.csv')

data = final_df.copy()

data.head()

df = data.copy()

df['Date'] = pd.to_datetime(df['Date'])

df.info()

plt.figure(figsize=(14, 7))
plt.plot(df['Date'], df['Close'], label="Close Price", color='blue')
plt.title("Tesla Stock Prices Over Time")
plt.xlabel("Date")
plt.ylabel("Stock Price")
plt.legend()
plt.grid()
plt.show()

"""###Sentiment Scores over Time
Analyze the sentiment distribution and trends
"""

import seaborn as sns

plt.figure(figsize=(14, 7))
plt.plot(df['Date'], df['Sentiment_Score'], label="Sentiment Score", color='green')
plt.axhline(y=0, color='red', linestyle='--', label="Neutral Sentiment")
plt.title("Sentiment Scores Over Time")
plt.xlabel("Date")
plt.ylabel("Sentiment Score")
plt.legend()
plt.grid()
plt.show()

# Distribution of sentiment scores
sns.histplot(df['Sentiment_Score'], bins=50, kde=True, color='purple')
plt.title("Distribution of Sentiment Scores")
plt.xlabel("Sentiment")
plt.ylabel("Frequency")
plt.show()

df.describe

"""###Feature Engineering for Strategies
1. Mean Reversion

Logic: Prices revert to their mean over time. We calculate z-scores for price deviations from moving averages.

2. Momentum

Logic: Stocks with positive momentum are likely to continue trending upwards.

3. Sentiment-Driven Strategy

Logic: Positive sentiment is often correlated with price increases.
"""

df1=df.copy()

# Calculate moving averages
df1['ma_20'] = df1['Close'].rolling(window=20,min_periods=1).mean()
df1['z_score'] = (df1['Close'] - df1['ma_20']) / df1['Close'].rolling(window=7, min_periods=1).std()

# Plot z-scores
plt.figure(figsize=(14, 7))
plt.plot(df1['Date'], df1['z_score'], label="Z-Score", color='orange')
plt.axhline(y=1, color='red', linestyle='--', label="Overbought (Sell)")
plt.axhline(y=-1, color='green', linestyle='--', label="Oversold (Buy)")
plt.title("Z-Scores of Tesla Prices")
plt.xlabel("Date")
plt.ylabel("Z-Score")
plt.legend()
plt.grid()
plt.show()

fig, ax1 = plt.subplots(figsize=(14, 7))


ax1.plot(df1['Date'], df1['Close'], label="Stock Price", color='blue')
ax1.set_xlabel("Date")
ax1.set_ylabel("Stock Price ($)", color='blue')
ax1.tick_params(axis='y', labelcolor='blue')


ax2 = ax1.twinx()
ax2.plot(df1['Date'], df1['Sentiment_Score'], label="Sentiment Score", color='green', alpha=0.7)
ax2.axhline(y=0, color='red', linestyle='--', label="Neutral Sentiment")
ax2.set_ylabel("Sentiment Score", color='green')
ax2.tick_params(axis='y', labelcolor='green')


fig.suptitle("Sentiment and Tesla Prices")
fig.tight_layout()
ax1.legend(loc="upper left")
ax2.legend(loc="upper right")

plt.grid()
plt.show()

# # Calculate momentum
            # df1['momentum'] = df1['Close'] - df1['Close'].shift(10)

            # # Plot momentum vs sentiment score

            # fig, ax1 = plt.subplots(figsize=(14, 7))
            # ax1.plot(df1['Date'], df1['momentum'], label="Momentum", color='blue')
            # ax1.set_xlabel("Date")
            # ax1.set_ylabel("Momentum", color='blue')
            # ax1.tick_params(axis='y', labelcolor='blue')
            # ax2 = ax1.twinx()
            # ax2.plot(df1['Date'], df1['Sentiment_Score'], label="Sentiment Score", color='green', alpha=0.7)
            # ax2.axhline(y=0, color='red', linestyle='--', label="Neutral Sentiment")
            # ax2.set_ylabel("Sentiment Score", color='green')
            # ax2.tick_params(axis='y', labelcolor='green')
            # fig.suptitle("Momentum vs Sentiment Score")
            # fig.tight_layout()
            # ax1.legend(loc="upper left")
            # ax2.legend(loc="upper right")

            # plt.grid()
            # plt.show()

# Experiment with multiple lags for future returns
for lag in [1, 3, 7, 14]:
    df1[f'future_returns_lag_{lag}'] = df1['Close'].shift(-lag) / df1['Close'] - 1

# Include additional features
df1['sentiment_volatility'] = df1['Sentiment_Score'].rolling(window=3).std()
df1['lagged_sentiment'] = df1['Sentiment_Score'].shift(1)

# Compute correlation for all features
correlation = df1[['Sentiment_Score', 'sentiment_volatility', 'lagged_sentiment',
                   'future_returns_lag_1', 'future_returns_lag_3', 'future_returns_lag_7', 'future_returns_lag_14']].corr()

# Heatmap
import plotly.express as px
fig = px.imshow(
    correlation,
    text_auto=".2f",
    color_continuous_scale=px.colors.sequential.RdBu,
    labels=dict(color="Correlation"),
    title="Correlation Matrix with Improved Features"
)
fig.show()

"""**- Log-transforming skewed features is a common technique to reduce the impact of outliers and normalize the data distribution.**"""

# Check for skewness before applying log-transformation of scores

from scipy.stats import skew

skew_value = skew(df1['Sentiment_Score'].dropna())
print(f"Skewness: {skew_value}")

!pip install dash plotly
!pip install jupyter-dash

"""### Interactive Dashbaord using Dash"""

import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import plotly.graph_objects as go
import pandas as pd



app = dash.Dash(__name__)

# Layout
app.layout = html.Div([
    html.H1("Tesla Stock Price and Sentiment Dashboard", style={'text-align': 'center'}),

    dcc.Graph(id="stock-sentiment-plot"),

    html.Div([
        html.Label("Select Date Range:"),
        dcc.DatePickerRange(
            id="date-picker",
            start_date=df1['Date'].min(),
            end_date=df1['Date'].max(),
            display_format="YYYY-MM-DD"
        )
    ], style={'margin': '30px'})
])


@app.callback(
    Output("stock-sentiment-plot", "figure"),
    Input("date-picker", "start_date"),
    Input("date-picker", "end_date")
)
def update_graph(start_date, end_date):
    filtered_df = df1[(df1['Date'] >= start_date) & (df1['Date'] <= end_date)]


    fig = go.Figure()

    # Stock price
    fig.add_trace(go.Scatter(
        x=filtered_df['Date'], y=filtered_df['Close'],
        mode='lines', name='Stock Price',
        line=dict(color='blue')
    ))

    # Sentiment score
    fig.add_trace(go.Scatter(
        x=filtered_df['Date'], y=filtered_df['Sentiment_Score'],
        mode='lines', name='Sentiment Score',
        line=dict(color='green'), opacity=0.7, yaxis='y2'
    ))


    fig.update_layout(
        title="Stock Price and Sentiment Over Time",
        xaxis_title="Date",
        yaxis=dict(title="Stock Price ($)", titlefont=dict(color='blue'), tickfont=dict(color='blue')),
        yaxis2=dict(title="Sentiment Score", overlaying="y", side="right", titlefont=dict(color='green'), tickfont=dict(color='green')),
        legend=dict(x=0, y=1.2, orientation="h"),
        margin=dict(l=40, r=40, t=40, b=40)
    )
    return fig


app.run_server(mode='inline')

import torch
import torch.nn as nn
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split


def preprocess_data_zscore(df, feature_columns, target_column, sequence_length, exclude_columns=[]):
    """
    Preprocess the data using z-score normalization.
    """
    feature_scaler = StandardScaler()
    target_scaler = StandardScaler()

    columns_to_scale = [col for col in feature_columns if col not in exclude_columns]
    df[columns_to_scale] = feature_scaler.fit_transform(df[columns_to_scale])
    df[target_column] = target_scaler.fit_transform(df[[target_column]])


    # Create sequences for LSTM
    X, y = [], []
    for i in range(len(df) - sequence_length):
        X.append(df[feature_columns].iloc[i:i+sequence_length].values)
        y.append(df[target_column].iloc[i+sequence_length])

    X = torch.tensor(np.array(X), dtype=torch.float32)
    y = torch.tensor(np.array(y), dtype=torch.float32)
    return X, y, feature_scaler, target_scaler

# Drop rows with NaN values
df1 = df1.dropna()


sequence_length = 30
feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume',
                   'ma_20', 'z_score', 'future_returns_lag_1', 'future_returns_lag_3', 'sentiment_volatility']
target_column = 'future_returns_lag_7'

exclude_columns = ['sentiment_volatility', 'Sentiment_Score']

# Prepare data
X, y, scaler, target_scaler = preprocess_data_zscore(df1, feature_columns, target_column, sequence_length, exclude_columns)

# Train-test split
train_size = int(0.8 * len(X))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

train_dataset = torch.utils.data.TensorDataset(X_train, y_train)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)

import torch
import torch.nn as nn
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split



class LSTMModel(nn.Module):
  def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout=0.2):
    super(LSTMModel, self).__init__()
    self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)
    self.fc = nn.Linear(hidden_dim, output_dim)

  def forward(self, x):
    out, _ = self.lstm(x)  # LSTM output
    out = self.fc(out[:, -1, :])  # Output of the last time step
    return out


input_dim = len(feature_columns)  # Number of input features
hidden_dim = 64
output_dim = 1  # {sentiment 7 day}: target variable
num_layers = 2
dropout = 0.2

# Define Training Process
# def train_model(model, train_loader, criterion, optimizer, num_epochs):
#   for epoch in range(num_epochs):
#     model.train()
#     train_loss = 0
#     for X_batch, y_batch in train_loader:
#         optimizer.zero_grad()
#         y_pred = model(X_batch)
#         loss = criterion(y_pred, y_batch.unsqueeze(1))
#         loss.backward()
#         optimizer.step()
#         train_loss += loss.item()
#     print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss / len(train_loader):.4f}")

def train_model(model, train_loader, criterion, optimizer, num_epochs):
  for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()

        # Check for NaNs
        if torch.isnan(X_batch).any() or torch.isnan(y_batch).any():
            print("NaNs detected in data!")
            continue

        y_pred = model(X_batch)


        if torch.isnan(y_pred).any():
            print("NaNs detected in predictions!")
            continue

        loss = criterion(y_pred, y_batch.unsqueeze(1))
        loss.backward()


        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()
        train_loss += loss.item()

    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss / len(train_loader):.4f}")



# Initialize Model, Loss, Optimizer
model = LSTMModel(input_dim, hidden_dim, output_dim, num_layers, dropout)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Train Model
num_epochs = 50
train_model(model, train_loader, criterion, optimizer, num_epochs)

# Evaluate Model
model.eval()
with torch.no_grad():
    y_pred = model(X_test).squeeze()
    y_pred = target_scaler.inverse_transform(y_pred.numpy().reshape(-1, 1)).flatten()
    y_actual = target_scaler.inverse_transform(y_test.numpy().reshape(-1, 1)).flatten()

# Calculate Metrics
rmse = np.sqrt(np.mean((y_pred - y_actual) ** 2))
directional_accuracy = np.mean((y_pred > 0) == (y_actual > 0))

print(f"Test RMSE: {rmse:.4f}")
print(f"Directional Accuracy: {directional_accuracy:.2%}")

# Trading Strategy
# def generate_signals(predictions, threshold=0.02):
#     signals = []
#     for pred in predictions:
#         if pred > threshold:
#             signals.append("Buy")
#         elif pred < -threshold:
#             signals.append("Sell")
#         else:
#             signals.append("Hold")
#     return signals

# signals = generate_signals(y_pred)
# print("Generated Signals:", signals[:10])

"""### Trend following strategy"""

# # Parameters for the strategy
# threshold = 0.02  # 2% price change threshold
# initial_balance = 10000  # Starting capital
# position = 0  # Current position (0 = no position, 1 = holding stock)
# balance = initial_balance  # Remaining balance
# portfolio = []  # Track portfolio value

# # Simulated Trading
# for i in range(len(y_pred) - 1):
#     predicted_change = (y_pred[i+1] - y_pred[i]) / y_pred[i]
#     actual_change = (y_actual[i+1] - y_actual[i]) / y_actual[i]
#     current_price = y_actual[i]

#     if position == 0:  # No position
#         if predicted_change > threshold:  # Buy signal
#             position = balance / current_price  # Buy as many shares as possible
#             balance = 0  # Deduct from cash
#             print(f"Buying at {current_price:.2f}, Position: {position:.2f} shares")
#         elif predicted_change < -threshold:  # Short-sell signal (if allowed)
#             position = -balance / current_price
#             balance = 0
#             print(f"Short-selling at {current_price:.2f}, Position: {position:.2f} shares")

#     elif position > 0:  # Holding long position
#         if predicted_change < -threshold:  # Sell signal
#             balance = position * current_price  # Sell all shares
#             position = 0
#             print(f"Selling at {current_price:.2f}, Balance: {balance:.2f}")

#     elif position < 0:  # Holding short position
#         if predicted_change > threshold:  # Cover short position
#             balance = -position * current_price
#             position = 0
#             print(f"Covering short at {current_price:.2f}, Balance: {balance:.2f}")

#     # Track portfolio value
#     portfolio.append(balance + (position * current_price if position != 0 else 0))

# # Final Portfolio Value
# final_balance = balance + (position * y_actual[-1] if position != 0 else 0)
# print(f"Final Portfolio Value: {final_balance:.2f}")

# # Plotting the portfolio value over time
# plt.figure(figsize=(10, 6))
# plt.plot(portfolio, label='Portfolio Value', color='blue')
# plt.xlabel('Time Step')
# plt.ylabel('Portfolio Value')
# plt.title('Trading Strategy Portfolio Performance Over Time')
# plt.legend()
# plt.show()

"""### Stop-Loss / Take-Profit Strategy
1.   Buy (or Short) based on the model's prediction.
2.   Set a Stop-Loss level, typically a percentage below the entry price for a long position and above the entry price for a short position.
3. Set a Take-Profit level, typically a percentage above the entry price for a long position and below the entry price for a short position.
4. Exit the position automatically when either the Stop-Loss or Take-Profit level is hit.
5. Track the portfolio value over time.

"""

import numpy as np
import matplotlib.pyplot as plt

# Parameters for the strategy
threshold = 0.02
initial_balance = 10000  # Starting capital
stop_loss_pct = 0.03  # 3% stop-loss
take_profit_pct = 0.06  # 6% take-profit
position = 0  # Current position (0 = no position, 1 = holding stock, -1 = short position)
balance = initial_balance
portfolio = []  # Track portfolio value


for i in range(len(y_pred) - 1):
    predicted_change = (y_pred[i+1] - y_pred[i]) / y_pred[i]
    actual_change = (y_actual[i+1] - y_actual[i]) / y_actual[i]
    current_price = y_actual[i]


    if position == 0:
        if predicted_change > threshold:  # Buy signal
            entry_price = current_price
            position = balance / entry_price  # Buy as many shares as possible
            balance = 0  # Deduct from cash
            stop_loss_price = entry_price * (1 - stop_loss_pct)  # Set stop-loss
            take_profit_price = entry_price * (1 + take_profit_pct)  # Set take-profit
            print(f"Buying at {current_price:.2f}, Position: {position:.2f} shares, Stop-Loss: {stop_loss_price:.2f}, Take-Profit: {take_profit_price:.2f}")
        elif predicted_change < -threshold:  # Short-sell signal
            entry_price = current_price
            position = -balance / entry_price  # Short-sell the stock
            balance = 0
            stop_loss_price = entry_price * (1 + stop_loss_pct)  # Set stop-loss for short
            take_profit_price = entry_price * (1 - take_profit_pct)  # Set take-profit for short
            print(f"Short-selling at {current_price:.2f}, Position: {position:.2f} shares, Stop-Loss: {stop_loss_price:.2f}, Take-Profit: {take_profit_price:.2f}")


    elif position > 0:
        if current_price <= stop_loss_price:  # Stop-loss hit
            balance = position * current_price  # Sell all shares at stop-loss price
            position = 0
            print(f"Selling at {current_price:.2f} (Stop-Loss), Balance: {balance:.2f}")
        elif current_price >= take_profit_price:  # Take-profit hit
            balance = position * current_price  # Sell all shares at take-profit price
            position = 0
            print(f"Selling at {current_price:.2f} (Take-Profit), Balance: {balance:.2f}")


    elif position < 0:
        if current_price >= stop_loss_price:  # Stop-loss hit for short
            balance = -position * current_price  # Cover short at stop-loss price
            position = 0
            print(f"Covering short at {current_price:.2f} (Stop-Loss), Balance: {balance:.2f}")
        elif current_price <= take_profit_price:  # Take-profit hit for short
            balance = -position * current_price  # Cover short at take-profit price
            position = 0
            print(f"Covering short at {current_price:.2f} (Take-Profit), Balance: {balance:.2f}")

    # Track portfolio value
    portfolio.append(balance + (position * current_price if position != 0 else 0))


final_balance = balance + (position * y_actual[-1] if position != 0 else 0)
print(f"Final Portfolio Value: {final_balance:.2f}")


plt.plot(portfolio)
plt.title("Portfolio Value Over Time")
plt.xlabel("Time Steps")
plt.ylabel("Portfolio Value ($)")
plt.show()

Profits = final_balance - initial_balance
print(f"Profits: {Profits:.2f}")



"""

> ***Change the code down here to something original. brainstorm it.***

"""

fig, ax = plt.subplots(figsize=(15,8))
ax.plot(stock_df['Date'], stock_df['Close'], color='#008B8B')
ax.set(xlabel="Date", ylabel="USD", title=f"{stock_name} Stock Price")
ax.xaxis.set_major_formatter(DateFormatter("%Y"))
plt.show()



(all_tweets["Stock Name"].unique())

